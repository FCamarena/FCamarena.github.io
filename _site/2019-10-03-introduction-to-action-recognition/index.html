<!DOCTYPE html>
<html lang="en">
  <!-- Beautiful Jekyll | MIT license | Copyright Dean Attali 2016 -->
  <head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, viewport-fit=cover">

  <title>Introduction to Action Recognition</title>

  <meta name="author" content="Fernando Camarena" />

  

  <link rel="alternate" type="application/rss+xml" title="FCamarena - My personal website" href="http://0.0.0.0:4000/feed.xml" />

  

  

  


  
    
      
  <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.6.0/css/font-awesome.min.css" />


    
  

  
    
      <link rel="stylesheet" href="/css/bootstrap.min.css" />
    
      <link rel="stylesheet" href="/css/bootstrap-social.css" />
    
      <link rel="stylesheet" href="/css/main.css" />
    
  

  
    
      <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" />
    
      <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" />
    
  

  

  

  

    <!-- Facebook OpenGraph tags -->
  

  
  <meta property="og:title" content="Introduction to Action Recognition" />
  

   
  <meta property="og:description" content="When I was a child I dreamed about a world where machines adquire “super-powers” (See image 1.0). Today, I’m working hard to make that dream a reality… but how? Maybe, you have heard about Artificial Intelligence (AI), but what about Computer Vision?. That sound a very creepy thing but its...">
  


  <meta property="og:type" content="website" />

  
  <meta property="og:url" content="http://0.0.0.0:4000/2019-10-03-introduction-to-action-recognition/" />
  <link rel="canonical" href="http://0.0.0.0:4000/2019-10-03-introduction-to-action-recognition/" />
  

  
  <meta property="og:image" content="http://0.0.0.0:4000/img/avatar-icon.png" />
  


  <!-- Twitter summary cards -->
  <meta name="twitter:card" content="summary" />
  <meta name="twitter:site" content="@" />
  <meta name="twitter:creator" content="@" />

  
  <meta name="twitter:title" content="Introduction to Action Recognition" />
  

  
  <meta name="twitter:description" content="When I was a child I dreamed about a world where machines adquire “super-powers” (See image 1.0). Today, I’m working hard to make that dream a reality… but how? Maybe, you have heard about Artificial Intelligence (AI), but what about Computer Vision?. That sound a very creepy thing but its...">
  

  
  <meta name="twitter:image" content="http://0.0.0.0:4000/img/avatar-icon.png" />
  

  

  

</head>


  <body>

    

  
    <nav class="navbar navbar-default navbar-fixed-top navbar-custom">
  <div class="container-fluid">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#main-navbar">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button><a class="navbar-brand" href="http://0.0.0.0:4000/">FCamarena</a></div>

    <div class="collapse navbar-collapse" id="main-navbar">
      <ul class="nav navbar-nav navbar-right">
          <li><a href="/aboutme">About Me</a></li>
          <li><a href="">Projects</a></li></ul>
    </div>

    <!-- <div class="avatar-container">
      <div class="avatar-img-border">
        <a href="http://0.0.0.0:4000/">
          <img class="avatar-img" src="/img/avatar-icon.png" />
      </a>
      </div>
    </div> -->

	<!-- 
	<div class="avatar-container">
	  <div class="avatar-img-border">
	    <a href="http://0.0.0.0:4000/">
	      <img class="avatar-img" src="/img/ComputerVision.png" />
		</a>
	  </div>
	</div>
	 -->

  </div>
</nav>


    <!-- TODO this file has become a mess, refactor it -->





<header class="header-section ">

<div class="intro-header no-img">
  <div class="container">
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <div class="post-heading">
          <h1>Introduction to Action Recognition</h1>
		  
		  
		  
		  <span class="post-meta">Posted on October 3, 2019</span>
		  
        </div>
      </div>
    </div>
  </div>
</div>
</header>





<div class="container">
  <div class="row">
    <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">

      

      <article role="main" class="blog-post">
        <p>When I was a child I dreamed about a world where machines adquire “super-powers” (See image 1.0). 
Today, I’m working hard to make that dream a reality… but how?</p>

<p><img src="/img/ComputerVision.png" alt="super machine" class="img-responsive" />
Maybe, you have heard about Artificial Intelligence (AI), but what about Computer Vision?. That sound a very creepy thing but its only aims is to give a computer eyes.</p>

<p>Computer Vision has a lot of application. Nevertheless, in this post, I’ll only focus on Action Recognition, which in simple words is to determinme what a subject is doing in a given video. Yes, like Big brother (See image 2.0)</p>

<p>— A crazy ilustration about computer vision —-</p>

<p>This is not a trivial task (Fortunaly?) but possible through  Machine Learning and Computer Vision techniques. Let’s begin with a brief explanation about what an action is.</p>

<h3 id="human-activity-recognition">Human Activity Recognition</h3>
<p>Human Activity Recognition (HAR)[REF] can be categorized into 3 levels: Gestures, Actions and Interactions. A gesture is an atomic movement, an action is a sequence of gestures with an associated message, and interactions are actions in which two or more agents are involved. A more compresive explanation is showed in figure 3.0</p>

<p>— HAR: A very simple image that explain it—–</p>
<h3 id="action-representation">Action Representation</h3>
<p>Action Recognition is formed by 2 steps: Action representation and Action Classification. The first one is how our computer will interpreted things and the second one is how the computer manipulate theses things in order to give an answer.</p>

<p>Action representation have been approached in 3 different ways: global, local and depth-based representation [REF]. Global representations have become obsolete due to high sensitivity to noise, occlusions and changing viewpoints. Depth-based representations require information about the object depth, which can be obtained by means of special cameras like Microsoft Kinect.</p>

<p>— Example of Global representation —-
— Example of depth based representation —</p>

<p>Local representations \cite{zhang2017review} are the most used and aim to describe a video through a collection of local descriptors sampled densely by placing points without any semantics or by locating points of interest. There are several approaches for  interest point location \cite{harris1988combined,laptev2005space,blank2005actions}, but one of the most widely used is the 3d space time interest points (STIP) \cite{laptev2005space}, which is an extension of the Harris detector \cite{harris1988combined}. Detecting interesting points may improve the performance, but there are some problems such as the number of points to be used and their representativeness \cite{zhang2017review}.</p>

<p>Once the interest points have been identified, the next step is to describe them. The most widely used are Scale-invariant feature transform (SIFT) \cite{lowe1999object,lowe2004distinctive}, 3D SIFT \cite{scovanner20073},  speed-up robust features (SURF) \cite{bay2008speeded}, 3D SURF \cite{willems2008efficient},  HOG \cite{dalal2005histograms}, HOF \cite{dalal2006human} and  MBH \citep{wang2011action}.</p>

<p>It is possible to combine several descriptors to improve performance such as the dense trajectory approach \cite{wang2013dense,wang2011action} that achieves high accuracy through the descriptors HOG, HOF, MBH, and the displacements of the trajectories (DT). Shi et al. \cite{shi2017sequential} present sequential deep trajectory descriptor (sDTD) to capture long-term motion information.</p>

<p>The next step is feature encoding, which the key idea is to discretize the entire space of local features extracted from a training set. Several works have been presented: Bag-of-words (BoW) \cite{chang2017improving}, fisher vector (FV) \cite{perronnin2010improving}, stacked fisher vector (SFV) \cite{peng2014action}, vector quantization (VQ) \cite{sivic2003video}, vector of locally aggregated descriptos (VLAD) \cite{jegou2010aggregating}, super vector encoding (SVC) \cite{zhou2010image}. Acording to \cite{zhang2017review} the best performance is achieved by using Dense trajectories with SFV.</p>

<h3 id="action-classification">Action Classification</h3>

<p>Action classification \cite{zhang2017review} has been carried out using 3 main approaches: template-based, generative and discriminatory methods. Template-based methods are the simplest and obtain a result through comparisons with a pre-defined set of templates. In generative methods use probabilistic approaches and discriminative ones uses machine learning techniques like support vector machines (SVM) \cite{ng2002discriminative}, conditional Random Fields (CRF) \cite{vail2007conditional} and deep learning architectures \cite{zhang2017review}.</p>

<p>On the other hand, deep learning architectures \cite{zhang2017review} like deep neural networks (DNNs) \cite{berlin2016human,huang2017deep}, convolutional neural networks (CNNs) \cite{mo2016human,simonyan2014two,li2016vlad3, gkioxari2015finding} and recurrent neural networks (RNNs) \cite{veeriah2015differential,du2015hierarchical} have achieved high accuracy values and even many of them perform the action recognition task in real time \cite{luo2018fast,zhang2016real}. Unlike traditional machine learning methods, deep neural networks has the ability to learn representations automatically.</p>

<p>State-of-the-art methods focuses on improving classification performance by combining CNN features with hand-crafted features. Li et al. \cite{li2016vlad3} proposed to combine CNN with VLAD to caputre mid-range and long-range dynamics. Wang et al. \cite{wang2015action} presented the deep-convolutional descriptor which combine dense trajectories with CNN features. Chéron et al. \cite{cheron2015p} introduced a Pose-based Convolutional Neural Network descriptor that proved that CNN approaches are complementary to Hand-crafted approaches.</p>

<p>our work follows the path of combining CNN features with traditional methods, in our case we explore the use of CNN for the generation of interest points.</p>

<p>Figure 4 sumarized all the explained:</p>

<p>————-Super Figure—</p>


      </article>

      

      
        <!-- Check if any share-links are active -->





      

      <ul class="pager blog-pager">
        
        
      </ul>

      
        <div class="disqus-comments">
          
        </div>
          
        <div class="staticman-comments">
          

        </div>
        <div class="justcomments-comments">
          
        </div>
      
    </div>
  </div>
</div>


    <footer>
  <div class="container beautiful-jekyll-footer">
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <ul class="list-inline text-center footer-links"><li><a href="mailto:fernando@camarenat.com" title="Email me"><span class="fa-stack fa-lg" aria-hidden="true">
                  <i class="fa fa-circle fa-stack-2x"></i>
                  <i class="fa fa-envelope fa-stack-1x fa-inverse"></i>
                </span>
                <span class="sr-only">Email me</span>
              </a>
            </li><li><a href="https://github.com/FCamarena" title="GitHub"><span class="fa-stack fa-lg" aria-hidden="true">
                  <i class="fa fa-circle fa-stack-2x"></i>
                  <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                </span>
                <span class="sr-only">GitHub</span>
              </a>
            </li></ul>
      <p class="copyright text-muted">
      Fernando Camarena
      &nbsp;&bull;&nbsp;
      2019

      
      &nbsp;&bull;&nbsp;
      <a href="http://0.0.0.0:4000/">fernando@camarenat.com</a>
      

      
      </p>
          <!-- Please don't remove this, keep my open source work credited :) -->
    <!-- <p class="theme-by text-muted">
      Theme by
      <a href="https://deanattali.com/beautiful-jekyll/">beautiful-jekyll</a>
    </p> -->
      </div>
    </div>
  </div>
</footer>

  
    


  
    <!-- doing something a bit funky here because I want to be careful not to include JQuery twice! -->
    
      <script>
      	if (typeof jQuery == 'undefined') {
          document.write('<script src="/js/jquery-1.11.2.min.js"></scr' + 'ipt>');
      	}
      </script>
    
  
    <!-- doing something a bit funky here because I want to be careful not to include JQuery twice! -->
    
      <script src="/js/bootstrap.min.js"></script>
    
  
    <!-- doing something a bit funky here because I want to be careful not to include JQuery twice! -->
    
      <script src="/js/main.js"></script>
    
  






  
  </body>
</html>
